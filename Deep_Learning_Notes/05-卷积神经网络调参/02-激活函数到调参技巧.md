![1593334232135](assets/1593334232135.png)

![1593334361698](assets/1593334361698.png)

- 稀疏数据举例：广告点击率预估业务
- Adam容易快速收敛

![1593334609607](assets/1593334609607.png)

![1593335342069](assets/1593335342069.png)

- Sigmoid函数输出均值是0.5，输出均值非0的话对于神经网络的学习是不友好的
- 梯度的反向传播是需要之前的梯度乘以当前的梯度才能传给下一层的梯度，Sigmoid对于层次比较深的网络会导致底层的参数得不到更新

![1593335425174](assets/1593335425174.png)

![1593335482587](assets/1593335482587.png)

![1593335602279](assets/1593335602279.png)

![1593335630702](assets/1593335630702.png)

![1593335682724](assets/1593335682724.png)

![1593335803484](assets/1593335803484.png)

- 使用Relu要小心的设置learning rate，因为relu的导数会比较大，sigmoid确实是计算太多，训练太慢

![1593336511363](assets/1593336511363.png)

- 一个网路如果初始化好的话，那么它可以既训练得快，又可以达到一个更好的效果
- 在多层网络中，如果把所有的参数都设置为0，那么在第一层的时候，不管输入是什么，输出都会是0，而第一层的输出又是第二层的输入，然后第二层所有的0去乘以参数的0得到的还是0，而多层网络的梯度和这一层网络的激活值是非常有关系的，如果激活值是0的话，那么它的梯度也都是0，因为梯度是0，而参数又是0，这样在多层网络中，会导致所有的梯度都会是0，而且因为参数也是0，所以参数也得不到更新，就会一直都是0

![1593339044237](assets/1593339044237.png)

- 激活值就是神经元经过激活函数之后的那个输出，激活值不一样就代表着它最后的那个梯度可能是不一样的

![1593339658179](assets/1593339658179.png)

- 之所以出现上图这种情况，就是因为方差设置的太小了

![1593339832131](assets/1593339832131.png)

![1593339925571](assets/1593339925571.png)

![1593340068402](assets/1593340068402.png)

![1593340094026](assets/1593340094026.png)

- 上图这种初始化方法也是何恺明提出的，也就是ResNet的提出者

![1593340376240](assets/1593340376240.png)

- 对于不同的激活函数，会有不同的初始化方法使得它的初始化效果会比较好
- “批”就是在它的每一个batch上去做Normalization，这样的做法会导致一个问题，就是当数据量非常非常大的时候，我的每一个batch并不能反应整个数据的分布
- 批归一化是在卷积神经网络中经常用到的一种技术，这种技术可以使得每一层的激活值的分布变得比较统一，使得神经网络变得更易于学习
- 在数据没有那么多的情况下，卷积神经网络就容易过拟合，如果要防止这种情况，就需要使得我们的数据变多
- 数据增强中的归一化就是把数据归一化到一定的区间内，使得网络更容易学习

![1593349476452](assets/1593349476452.png)

![1593349494224](assets/1593349494224.png)

![1593349620309](assets/1593349620309.png)

![1593349809809](assets/1593349809809.png)

- 深度学习的网络都是由大数据给喂出来的
- 正则化是机器学习中防止过拟合的一种方法，一个神经网络的容量是由它的参数个数所决定的，当参数个数过多的时候，就容易发生过拟合情况，在损失函数上加上一个正则化项，正则化项可以是参数的平方值或者是参数的绝对值（L2正则和L1正则），准确地说模型的容量和非0参数的个数是有关系的
- 梯度的分布决定了神经网络训练的程度是什么样子，当所有梯度都比较小的时候，说明神经网络快训练完了，当梯度比较大时，说明神经网络还需要一段时间进行训练才能达到收敛的效果

![1593350800655](assets/1593350800655.png)

![1593350832758](assets/1593350832758.png)

- 上图中第一张图表示，首先它并没有收敛，其次是它学习到的梯度并没有真正的反映数据的规律，原因可能是learning rate设置的比较大

![1593351649954](assets/1593351649954.png)

![1593351746988](assets/1593351746988.png)

