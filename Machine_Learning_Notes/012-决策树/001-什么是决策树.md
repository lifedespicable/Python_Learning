![1571750119808](assets/1571750119808.png)

![1571750136856](assets/1571750136856.png)

- 下图的决策树的深度为 3， 因为最多经过 3 次判断就可以将数据进行一个相应的分类

![1571750332143](assets/1571750332143.png)

- entropy 就是 熵 的意思

![1571752086888](assets/1571752086888.png)

![1571752188546](assets/1571752188546.png)

![1571752299276](assets/1571752299276.png)

![1571752347443](assets/1571752347443.png)

